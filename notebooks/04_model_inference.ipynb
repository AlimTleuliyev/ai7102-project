{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5efa0e4",
   "metadata": {},
   "source": [
    "# Part 4: Model Inference Details üî¨\n",
    "\n",
    "## üìö Learning Goals:\n",
    "1. Understand `calc_surprisal_hf.py` **line-by-line**\n",
    "2. See how **batching** works (processing multiple pieces at once)\n",
    "3. Understand **padding** and **masking**\n",
    "4. See how **target spans** extract the right surprisals\n",
    "5. Understand **PPL** (Perplexity) calculation\n",
    "6. Run actual code on our data!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Big Picture:\n",
    "\n",
    "We've learned:\n",
    "- Part 1: What files exist (INPUT ‚Üí OUTPUT)\n",
    "- Part 2: Why 31 JSONs (context modification)\n",
    "- Part 3: What is surprisal (mathematical definition)\n",
    "\n",
    "**Now**: How does `calc_surprisal_hf.py` actually compute ALL those surprisals?\n",
    "\n",
    "### The Script's Job:\n",
    "```\n",
    "INPUT:  ngram_2-contextfunc_delete.json\n",
    "        (contains ~212,000 context-target pairs)\n",
    "        \n",
    "OUTPUT: scores.json\n",
    "        (contains ~212,000 surprisal values)\n",
    "        + eval.txt (perplexity)\n",
    "```\n",
    "\n",
    "**Challenge**: Can't process all 212K pieces one-by-one (too slow!)\n",
    "\n",
    "**Solution**: **BATCHING** - process many pieces at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0754b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7db669",
   "metadata": {},
   "source": [
    "## üìÇ Step 1: Load Data\n",
    "\n",
    "Let's load one of our JSON files and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb9cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../data/DC/ngram_2-contextfunc_delete.json\n",
      "Number of articles: 20\n",
      "\n",
      "First article ID: 1\n",
      "Number of pieces in first article: 2573\n",
      "\n",
      "================================================================================\n",
      "FIRST 3 PIECES FROM ARTICLE 1:\n",
      "================================================================================\n",
      "\n",
      "Piece 0:\n",
      "  Context: ''\n",
      "  Target:  'Are'\n",
      "\n",
      "Piece 1:\n",
      "  Context: 'Are'\n",
      "  Target:  'tourists'\n",
      "\n",
      "Piece 2:\n",
      "  Context: 'tourists'\n",
      "  Target:  'ent iced'\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_path = '../data/DC/ngram_2-contextfunc_delete.json'\n",
    "article2piece = json.load(open(data_path))\n",
    "\n",
    "print(f\"Loaded: {data_path}\")\n",
    "print(f\"Number of articles: {len(article2piece)}\")\n",
    "print(f\"\\nFirst article ID: {list(article2piece.keys())[0]}\")\n",
    "print(f\"Number of pieces in first article: {len(article2piece['1'])}\")\n",
    "\n",
    "# Look at first 3 pieces\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIRST 3 PIECES FROM ARTICLE 1:\")\n",
    "print(\"=\"*80)\n",
    "for i, (context, target) in enumerate(article2piece['1'][:3]):\n",
    "    context_clean = context.replace('‚ñÅ', ' ').strip()\n",
    "    target_clean = target.replace('‚ñÅ', ' ').strip()\n",
    "    print(f\"\\nPiece {i}:\")\n",
    "    print(f\"  Context: '{context_clean}'\")\n",
    "    print(f\"  Target:  '{target_clean}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c86c63",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Load Model\n",
    "\n",
    "Same GPT-2 model we used in Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea0da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded gpt2 on mps\n",
      "üìä Parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Check device\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create loss function (used to compute cross-entropy = surprisal)\n",
    "loss_fct = CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {model_name} on {device}\")\n",
    "print(f\"üìä Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dd122",
   "metadata": {},
   "source": [
    "## üîç Step 3: Process ONE Piece (Understand the Logic)\n",
    "\n",
    "Before batching, let's process a single piece to understand what happens.\n",
    "\n",
    "### The Process:\n",
    "1. **Tokenize context and target**\n",
    "2. **Identify target span** (where target tokens are in the sequence)\n",
    "3. **Run model forward pass**\n",
    "4. **Extract surprisals** for target tokens only\n",
    "5. **Sum them** to get word-level surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a59aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing single piece:\n",
      "Context: ''\n",
      "Target:  'Are'\n",
      "\n",
      "Full text: '<|endoftext|>Are'\n",
      "\n",
      "Tokenization:\n",
      "  Context tokens: ['<|endoftext|>']\n",
      "  Target tokens:  ['Are']\n",
      "  Full tokens:    ['<|endoftext|>', 'Are']\n",
      "\n",
      "Target span: (1, 1)\n",
      "  ‚Üí Tokens at positions 1 to 1\n",
      "  ‚Üí ['Are']\n"
     ]
    }
   ],
   "source": [
    "# Take first piece\n",
    "context, target = article2piece['1'][0]\n",
    "context_clean = context.replace('‚ñÅ', ' ').strip()\n",
    "target_clean = target.replace('‚ñÅ', ' ').strip()\n",
    "\n",
    "print(\"Processing single piece:\")\n",
    "print(f\"Context: '{context_clean}'\")\n",
    "print(f\"Target:  '{target_clean}'\")\n",
    "print()\n",
    "\n",
    "# Handle empty context (first word in sentence)\n",
    "if not context_clean:\n",
    "    context_for_model = \"<|endoftext|>\"  # GPT-2's BOS token\n",
    "    text = context_for_model + target_clean\n",
    "else:\n",
    "    context_for_model = context_clean\n",
    "    text = context_clean + target_clean\n",
    "\n",
    "print(f\"Full text: '{text}'\")\n",
    "print()\n",
    "\n",
    "# Tokenize\n",
    "encoded_context = tokenizer(context_for_model, return_tensors=\"pt\")\n",
    "encoded_target = tokenizer(target_clean, return_tensors=\"pt\")\n",
    "encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Tokenization:\")\n",
    "print(f\"  Context tokens: {tokenizer.convert_ids_to_tokens(encoded_context['input_ids'][0])}\")\n",
    "print(f\"  Target tokens:  {tokenizer.convert_ids_to_tokens(encoded_target['input_ids'][0])}\")\n",
    "print(f\"  Full tokens:    {tokenizer.convert_ids_to_tokens(encoded_text['input_ids'][0])}\")\n",
    "print()\n",
    "\n",
    "# Find target span\n",
    "start = len(encoded_context['input_ids'][0])\n",
    "target_len = len(encoded_target['input_ids'][0])\n",
    "target_span = (start, start + target_len - 1)\n",
    "\n",
    "print(f\"Target span: {target_span}\")\n",
    "print(f\"  ‚Üí Tokens at positions {target_span[0]} to {target_span[1]}\")\n",
    "print(f\"  ‚Üí {tokenizer.convert_ids_to_tokens(encoded_text['input_ids'][0][target_span[0]:target_span[1]+1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4ac20",
   "metadata": {},
   "source": [
    "## üéØ Step 4: Compute Surprisal for This Piece\n",
    "\n",
    "Now let's run the model and extract surprisal.\n",
    "\n",
    "### Key Insight: Input/Output Alignment\n",
    "```\n",
    "INPUT:  [token‚ÇÄ, token‚ÇÅ, token‚ÇÇ, token‚ÇÉ]  ‚Üê What model sees\n",
    "OUTPUT: [pred‚ÇÅ,  pred‚ÇÇ,  pred‚ÇÉ,  pred‚ÇÑ]  ‚Üê What model predicts\n",
    "GOLD:   [token‚ÇÅ, token‚ÇÇ, token‚ÇÉ, token‚ÇÑ]  ‚Üê Actual next tokens\n",
    "```\n",
    "\n",
    "Model at position `i` predicts token at position `i+1`.\n",
    "\n",
    "So we:\n",
    "- **Input IDs**: All tokens except last\n",
    "- **Gold IDs**: All tokens except first\n",
    "- **Loss**: Compares predictions to gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecf3f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/Output alignment:\n",
      "  Text IDs:  [50256, 8491]\n",
      "  Input IDs: [50256]  ‚Üê Model input\n",
      "  Gold IDs:  [8491]   ‚Üê Actual next tokens\n",
      "\n",
      "Logits shape: torch.Size([1, 1, 50257])\n",
      "  ‚Üí (batch_size=1, sequence_length=1, vocabulary_size=50257)\n",
      "\n",
      "Loss per token shape: torch.Size([1, 1])\n",
      "Loss values (in nats): [7.436365127563477]\n",
      "\n",
      "Target surprisals (nats): [7.436365127563477]\n",
      "Total surprisal (nats):   7.436\n",
      "Total surprisal (bits):   10.728\n",
      "\n",
      "üí° This single number (7.436 nats) goes into scores.json!\n"
     ]
    }
   ],
   "source": [
    "# Prepare input and gold\n",
    "text_ids = encoded_text['input_ids'][0]\n",
    "input_ids = text_ids[:-1]  # All except last\n",
    "gold_ids = text_ids[1:]    # All except first\n",
    "\n",
    "print(\"Input/Output alignment:\")\n",
    "print(f\"  Text IDs:  {text_ids.tolist()}\")\n",
    "print(f\"  Input IDs: {input_ids.tolist()}  ‚Üê Model input\")\n",
    "print(f\"  Gold IDs:  {gold_ids.tolist()}   ‚Üê Actual next tokens\")\n",
    "print()\n",
    "\n",
    "# Run model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids.unsqueeze(0).to(device))  # Add batch dimension\n",
    "    logits = outputs.logits  # Shape: (1, seq_len, vocab_size)\n",
    "    \n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"  ‚Üí (batch_size=1, sequence_length={logits.shape[1]}, vocabulary_size={logits.shape[2]})\")\n",
    "print()\n",
    "\n",
    "# Compute loss (cross-entropy = surprisal in nats, we'll convert to bits)\n",
    "loss_per_token = loss_fct(\n",
    "    logits.transpose(1, 2),  # CrossEntropyLoss expects (batch, vocab, seq)\n",
    "    gold_ids.unsqueeze(0).to(device)\n",
    ")\n",
    "\n",
    "print(f\"Loss per token shape: {loss_per_token.shape}\")\n",
    "print(f\"Loss values (in nats): {loss_per_token[0].cpu().tolist()}\")\n",
    "print()\n",
    "\n",
    "# Extract target surprisals\n",
    "# NOTE: target_span refers to text_ids positions, but loss is for input_ids\n",
    "# So we need to adjust by -1\n",
    "target_surprisals_nats = loss_per_token[0, target_span[0]-1:target_span[1]].cpu().tolist()\n",
    "target_surprisal_total = sum(target_surprisals_nats)\n",
    "\n",
    "print(f\"Target surprisals (nats): {target_surprisals_nats}\")\n",
    "print(f\"Total surprisal (nats):   {target_surprisal_total:.3f}\")\n",
    "print(f\"Total surprisal (bits):   {target_surprisal_total / np.log(2):.3f}\")\n",
    "print()\n",
    "print(f\"üí° This single number ({target_surprisal_total:.3f} nats) goes into scores.json!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e5229",
   "metadata": {},
   "source": [
    "## ü§î Wait... Why Use Loss Function Instead of Manual Calculation?\n",
    "\n",
    "**Great question!** In Part 3, we manually:\n",
    "1. Got logits\n",
    "2. Applied softmax ‚Üí probabilities\n",
    "3. Extracted probability of actual token\n",
    "4. Computed `-log2(prob)` ‚Üí surprisal\n",
    "\n",
    "**Here we use `CrossEntropyLoss`**. Why?\n",
    "\n",
    "### They're the SAME thing! Just more efficient.\n",
    "\n",
    "**CrossEntropyLoss** combines steps 2-4 into ONE operation:\n",
    "```python\n",
    "# What we did in Part 3 (manual):\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "prob_of_actual = probs[0, i, actual_token_id]\n",
    "surprisal = -torch.log(prob_of_actual)  # Natural log (nats)\n",
    "\n",
    "# What CrossEntropyLoss does (automatic):\n",
    "loss = CrossEntropyLoss(logits, actual_token_id)\n",
    "# Returns: -log(softmax(logits)[actual_token_id])\n",
    "# This IS surprisal! (in nats)\n",
    "```\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Aspect | Part 3 (Manual) | Part 4 (Loss Function) |\n",
    "|--------|----------------|----------------------|\n",
    "| Softmax | Explicit | Built-in (more stable) |\n",
    "| Log base | log‚ÇÇ (bits) | ln (nats) |\n",
    "| Speed | Slower | Faster (optimized) |\n",
    "| Numerical stability | Can have issues | Very stable |\n",
    "| Use case | Learning | Production |\n",
    "\n",
    "### Why Nats vs Bits?\n",
    "- **Nats**: Natural log (ln) - PyTorch default\n",
    "- **Bits**: Log base 2 (log‚ÇÇ) - information theory standard\n",
    "- **Conversion**: `bits = nats / ln(2) ‚âà nats √ó 1.443`\n",
    "\n",
    "### Numerical Stability\n",
    "CrossEntropyLoss is more stable because it uses the \"log-sum-exp trick\":\n",
    "```python\n",
    "# Naive (can overflow):\n",
    "softmax = exp(logit_i) / sum(exp(logit_j))\n",
    "\n",
    "# Stable (what PyTorch does):\n",
    "softmax = exp(logit_i - max(logits)) / sum(exp(logit_j - max(logits)))\n",
    "```\n",
    "\n",
    "**Bottom line**: Same math, just optimized for speed and stability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2d39b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Manual vs CrossEntropyLoss:\n",
      "================================================================================\n",
      "Target token ID: 8491\n",
      "Target token: 'Are'\n",
      "\n",
      "METHOD 1: Manual Calculation (Part 3 style)\n",
      "  1. Logits shape: torch.Size([50257])\n",
      "  2. After softmax ‚Üí probabilities\n",
      "  3. Probability of actual token: 0.000589\n",
      "  4. Surprisal (nats): -log(0.000589) = 7.4364\n",
      "  5. Surprisal (bits): 10.7284\n",
      "\n",
      "METHOD 2: CrossEntropyLoss (Part 4 style)\n",
      "  1. CrossEntropyLoss directly computes: 7.4364 nats\n",
      "  2. Convert to bits: 10.7284\n",
      "\n",
      "COMPARISON:\n",
      "  Manual (nats):  7.436365\n",
      "  Loss (nats):    7.436365\n",
      "  Difference:     0.0000000000\n",
      "\n",
      "  Manual (bits):  10.728407\n",
      "  Loss (bits):    10.728407\n",
      "\n",
      "‚úÖ They're the SAME! (tiny difference due to floating point precision)\n",
      "   CrossEntropyLoss just does it faster and more stably!\n"
     ]
    }
   ],
   "source": [
    "# Let's prove they're the same!\n",
    "print(\"Comparing Manual vs CrossEntropyLoss:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Take first target token\n",
    "first_target_pos = target_span[0] - 1  # Adjust for input/output offset\n",
    "first_target_id = gold_ids[first_target_pos].item()\n",
    "\n",
    "print(f\"Target token ID: {first_target_id}\")\n",
    "print(f\"Target token: '{tokenizer.decode([first_target_id])}'\")\n",
    "print()\n",
    "\n",
    "# Method 1: Manual (like Part 3)\n",
    "print(\"METHOD 1: Manual Calculation (Part 3 style)\")\n",
    "logit_for_position = logits[0, first_target_pos]  # Shape: (vocab_size,)\n",
    "probs = torch.softmax(logit_for_position, dim=-1)\n",
    "prob_of_actual = probs[first_target_id]\n",
    "surprisal_manual_nats = -torch.log(prob_of_actual)\n",
    "surprisal_manual_bits = surprisal_manual_nats / np.log(2)\n",
    "\n",
    "print(f\"  1. Logits shape: {logit_for_position.shape}\")\n",
    "print(f\"  2. After softmax ‚Üí probabilities\")\n",
    "print(f\"  3. Probability of actual token: {prob_of_actual:.6f}\")\n",
    "print(f\"  4. Surprisal (nats): -log({prob_of_actual:.6f}) = {surprisal_manual_nats:.4f}\")\n",
    "print(f\"  5. Surprisal (bits): {surprisal_manual_bits:.4f}\")\n",
    "print()\n",
    "\n",
    "# Method 2: CrossEntropyLoss\n",
    "print(\"METHOD 2: CrossEntropyLoss (Part 4 style)\")\n",
    "surprisal_loss = loss_per_token[0, first_target_pos]\n",
    "surprisal_loss_bits = surprisal_loss / np.log(2)\n",
    "\n",
    "print(f\"  1. CrossEntropyLoss directly computes: {surprisal_loss:.4f} nats\")\n",
    "print(f\"  2. Convert to bits: {surprisal_loss_bits:.4f}\")\n",
    "print()\n",
    "\n",
    "# Compare\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"  Manual (nats):  {surprisal_manual_nats:.6f}\")\n",
    "print(f\"  Loss (nats):    {surprisal_loss:.6f}\")\n",
    "print(f\"  Difference:     {abs(surprisal_manual_nats - surprisal_loss):.10f}\")\n",
    "print()\n",
    "print(f\"  Manual (bits):  {surprisal_manual_bits:.6f}\")\n",
    "print(f\"  Loss (bits):    {surprisal_loss_bits:.6f}\")\n",
    "print()\n",
    "print(\"‚úÖ They're the SAME! (tiny difference due to floating point precision)\")\n",
    "print(\"   CrossEntropyLoss just does it faster and more stably!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c836317",
   "metadata": {},
   "source": [
    "## üì¶ Step 5: Understanding Batching\n",
    "\n",
    "Processing one piece at a time is **SLOW**. We have 212,000 pieces!\n",
    "\n",
    "### The Problem:\n",
    "- Pieces have different lengths\n",
    "- GPUs are fast when processing same-length sequences together\n",
    "\n",
    "### The Solution: Padding\n",
    "```\n",
    "Piece 1: [45, 23, 12, 89]           ‚Üí length 4\n",
    "Piece 2: [12, 34]                   ‚Üí length 2\n",
    "Piece 3: [78, 90, 23, 45, 67, 11]  ‚Üí length 6\n",
    "\n",
    "Pad to max length (6):\n",
    "Piece 1: [45, 23, 12, 89, -100, -100]  ‚Üê padded\n",
    "Piece 2: [12, 34, -100, -100, -100, -100]\n",
    "Piece 3: [78, 90, 23, 45, 67, 11]\n",
    "```\n",
    "\n",
    "Then use **attention mask** to ignore padding positions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d34a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 pieces (different lengths):\n",
      "  Piece 0: length 2 ‚Üí [50256, 8491]\n",
      "  Piece 1: length 4 ‚Üí [32, 1186, 454, 1023]\n",
      "  Piece 2: length 6 ‚Üí [83, 454, 1023, 298, 220, 3711]\n",
      "\n",
      "After padding:\n",
      "Padded input shape: torch.Size([3, 5])\n",
      "  Piece 0: [50256, -100, -100, -100, -100]\n",
      "  Piece 1: [32, 1186, 454, -100, -100]\n",
      "  Piece 2: [83, 454, 1023, 298, 220]\n",
      "\n",
      "Attention mask:\n",
      "  Piece 0: [1, 0, 0, 0, 0]\n",
      "  Piece 1: [1, 1, 1, 0, 0]\n",
      "  Piece 2: [1, 1, 1, 1, 1]\n",
      "\n",
      "üí° Model will IGNORE positions with mask=0!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate batching with 3 pieces\n",
    "pieces = article2piece['1'][:3]\n",
    "text_ids_list = []\n",
    "target_spans = []\n",
    "\n",
    "for context, target in pieces:\n",
    "    context_clean = context.replace('‚ñÅ', ' ').strip()\n",
    "    target_clean = target.replace('‚ñÅ', ' ').strip()\n",
    "    \n",
    "    if context_clean:\n",
    "        text = context_clean + target_clean\n",
    "    else:\n",
    "        text = \"<|endoftext|>\" + target_clean\n",
    "    \n",
    "    encoded_context = tokenizer(context_clean if context_clean else \"<|endoftext|>\", return_tensors=\"pt\")\n",
    "    encoded_target = tokenizer(target_clean, return_tensors=\"pt\")\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    start = len(encoded_context['input_ids'][0])\n",
    "    target_len = len(encoded_target['input_ids'][0])\n",
    "    target_span = (start, start + target_len - 1)\n",
    "    \n",
    "    text_ids_list.append(encoded_text['input_ids'][0])\n",
    "    target_spans.append(target_span)\n",
    "\n",
    "print(\"3 pieces (different lengths):\")\n",
    "for i, ids in enumerate(text_ids_list):\n",
    "    print(f\"  Piece {i}: length {len(ids)} ‚Üí {ids.tolist()}\")\n",
    "print()\n",
    "\n",
    "# Prepare for batching\n",
    "input_ids_list = [ids[:-1] for ids in text_ids_list]\n",
    "gold_ids_list = [ids[1:] for ids in text_ids_list]\n",
    "\n",
    "# Pad sequences\n",
    "pad_id = -100\n",
    "padded_input = torch.nn.utils.rnn.pad_sequence(\n",
    "    input_ids_list, batch_first=True, padding_value=pad_id\n",
    ")\n",
    "padded_gold = torch.nn.utils.rnn.pad_sequence(\n",
    "    gold_ids_list, batch_first=True, padding_value=pad_id\n",
    ")\n",
    "\n",
    "print(\"After padding:\")\n",
    "print(f\"Padded input shape: {padded_input.shape}\")\n",
    "print(f\"  Piece 0: {padded_input[0].tolist()}\")\n",
    "print(f\"  Piece 1: {padded_input[1].tolist()}\")\n",
    "print(f\"  Piece 2: {padded_input[2].tolist()}\")\n",
    "print()\n",
    "\n",
    "# Create attention mask (1 for real tokens, 0 for padding)\n",
    "attention_mask = (padded_input > -1).int()\n",
    "print(\"Attention mask:\")\n",
    "print(f\"  Piece 0: {attention_mask[0].tolist()}\")\n",
    "print(f\"  Piece 1: {attention_mask[1].tolist()}\")\n",
    "print(f\"  Piece 2: {attention_mask[2].tolist()}\")\n",
    "print()\n",
    "print(\"üí° Model will IGNORE positions with mask=0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9eb888",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Batched Forward Pass\n",
    "\n",
    "Now let's run the model on all 3 pieces at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b2363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batched forward pass...\n",
      "Logits shape: torch.Size([3, 5, 50257])\n",
      "  ‚Üí (batch_size=3, max_length=5, vocab_size=50257)\n",
      "\n",
      "Batched losses shape: torch.Size([3, 5])\n",
      "  ‚Üí (batch_size=3, max_length=5)\n",
      "\n",
      "Extracting surprisals for target spans:\n",
      "  Piece 0: span=(1, 1) ‚Üí surprisal=7.436 nats\n",
      "  Piece 1: span=(1, 3) ‚Üí surprisal=29.196 nats\n",
      "  Piece 2: span=(3, 5) ‚Üí surprisal=26.908 nats\n",
      "\n",
      "‚úÖ This is how batching speeds up computation!\n",
      "   Process 50-100 pieces at once instead of one-by-one!\n"
     ]
    }
   ],
   "source": [
    "# Replace padding with 0 (model can't handle -100 as input)\n",
    "padded_input_clean = torch.where(padded_input == pad_id, 0, padded_input)\n",
    "padded_gold_clean = torch.where(padded_gold == pad_id, 0, padded_gold)\n",
    "\n",
    "print(\"Running batched forward pass...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=padded_input_clean.to(device),\n",
    "        attention_mask=attention_mask.to(device)\n",
    "    )\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"  ‚Üí (batch_size={logits.shape[0]}, max_length={logits.shape[1]}, vocab_size={logits.shape[2]})\")\n",
    "print()\n",
    "\n",
    "# Compute loss for all pieces at once\n",
    "batched_losses = loss_fct(\n",
    "    logits.transpose(1, 2),\n",
    "    padded_gold_clean.to(device)\n",
    ")\n",
    "\n",
    "print(f\"Batched losses shape: {batched_losses.shape}\")\n",
    "print(f\"  ‚Üí (batch_size={batched_losses.shape[0]}, max_length={batched_losses.shape[1]})\")\n",
    "print()\n",
    "\n",
    "# Extract surprisals for each piece's target\n",
    "print(\"Extracting surprisals for target spans:\")\n",
    "for i, span in enumerate(target_spans):\n",
    "    target_losses = batched_losses[i, span[0]-1:span[1]].cpu().tolist()\n",
    "    total_surprisal = sum(target_losses)\n",
    "    print(f\"  Piece {i}: span={span} ‚Üí surprisal={total_surprisal:.3f} nats\")\n",
    "\n",
    "print(\"\\n‚úÖ This is how batching speeds up computation!\")\n",
    "print(\"   Process 50-100 pieces at once instead of one-by-one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0fb7f",
   "metadata": {},
   "source": [
    "## üìä Step 7: Perplexity (PPL) Calculation\n",
    "\n",
    "The script also outputs `eval.txt` with a **Perplexity** value.\n",
    "\n",
    "### What is Perplexity?\n",
    "- Measures how \"surprised\" the model is on average\n",
    "- Lower PPL = better predictions\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{PPL} = \\exp\\left(\\frac{1}{N} \\sum_{i=1}^{N} \\text{loss}_i\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ = total number of tokens\n",
    "- $\\text{loss}_i$ = cross-entropy loss for token $i$ (in nats)\n",
    "\n",
    "### Intuition:\n",
    "- PPL = 1: Perfect prediction (no surprise)\n",
    "- PPL = 100: On average, model is choosing from ~100 equally likely words\n",
    "- PPL = 1000: Much more uncertain\n",
    "\n",
    "**Note**: PPL is computed over ALL tokens (not just targets), but for simplicity we'll use target tokens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3c8a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity calculation:\n",
      "  Total tokens: 7\n",
      "  Mean loss (nats): 9.077\n",
      "  Perplexity: 8753.64\n",
      "\n",
      "üí° PPL=8753.64 means model is on average choosing from ~8754 equally likely words\n",
      "   (Lower is better - more confident predictions)\n"
     ]
    }
   ],
   "source": [
    "# Collect all losses from our 3 pieces\n",
    "all_losses = []\n",
    "for i, span in enumerate(target_spans):\n",
    "    target_losses = batched_losses[i, span[0]-1:span[1]].cpu().tolist()\n",
    "    all_losses.extend(target_losses)\n",
    "\n",
    "# Compute PPL\n",
    "mean_loss = np.mean(all_losses)\n",
    "perplexity = np.exp(mean_loss)\n",
    "\n",
    "print(\"Perplexity calculation:\")\n",
    "print(f\"  Total tokens: {len(all_losses)}\")\n",
    "print(f\"  Mean loss (nats): {mean_loss:.3f}\")\n",
    "print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "print()\n",
    "print(f\"üí° PPL={perplexity:.2f} means model is on average choosing from ~{perplexity:.0f} equally likely words\")\n",
    "print(\"   (Lower is better - more confident predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb0de2",
   "metadata": {},
   "source": [
    "## üîÑ Step 8: The Full Loop\n",
    "\n",
    "In the actual script, this process repeats for:\n",
    "- **All articles** in the JSON file\n",
    "- **All pieces** in each article\n",
    "- Using **batches of 50** (default) for efficiency\n",
    "\n",
    "### Pseudocode:\n",
    "```python\n",
    "for article in article2piece:\n",
    "    pieces = article2piece[article]\n",
    "    \n",
    "    # Process in batches of 50\n",
    "    for batch_start in range(0, len(pieces), 50):\n",
    "        batch = pieces[batch_start:batch_start+50]\n",
    "        \n",
    "        # 1. Tokenize all pieces in batch\n",
    "        # 2. Pad to same length\n",
    "        # 3. Run model forward pass\n",
    "        # 4. Extract surprisals for each piece\n",
    "        # 5. Save to article2scores[article]\n",
    "    \n",
    "# Save all results\n",
    "json.dump(article2scores, open('scores.json', 'w'))\n",
    "json.dump({'PPL': perplexity}, open('eval.txt', 'w'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ae909",
   "metadata": {},
   "source": [
    "## üéì Advanced: Sorting by Length\n",
    "\n",
    "One optimization in the script: **sort pieces by length before batching**.\n",
    "\n",
    "### Why?\n",
    "- Pieces of similar length ‚Üí less padding needed\n",
    "- Less padding ‚Üí faster computation, less memory\n",
    "\n",
    "### Example:\n",
    "**Without sorting:**\n",
    "```\n",
    "Batch 1: [length 50, length 10, length 45] ‚Üí max=50, lots of padding\n",
    "Batch 2: [length 12, length 48, length 15] ‚Üí max=48, lots of padding\n",
    "```\n",
    "\n",
    "**With sorting:**\n",
    "```\n",
    "Batch 1: [length 10, length 12, length 15] ‚Üí max=15, minimal padding\n",
    "Batch 2: [length 45, length 48, length 50] ‚Üí max=50, minimal padding\n",
    "```\n",
    "\n",
    "The script tracks original indices to restore order at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b87e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original order (by article position):\n",
      "  Piece 0: length 2\n",
      "  Piece 1: length 4\n",
      "  Piece 2: length 6\n",
      "\n",
      "Sorted order (by length):\n",
      "  Piece 0: length 2\n",
      "  Piece 1: length 4\n",
      "  Piece 2: length 6\n",
      "\n",
      "üí° After processing, results are restored to original order using indices!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate sorting optimization\n",
    "print(\"Original order (by article position):\")\n",
    "for i, ids in enumerate(text_ids_list):\n",
    "    print(f\"  Piece {i}: length {len(ids)}\")\n",
    "print()\n",
    "\n",
    "# Sort by length\n",
    "ids_span_idx = [(ids, span, i) for i, (ids, span) in enumerate(zip(text_ids_list, target_spans))]\n",
    "ids_span_idx_sorted = sorted(ids_span_idx, key=lambda x: len(x[0]))\n",
    "\n",
    "print(\"Sorted order (by length):\")\n",
    "for ids, span, orig_idx in ids_span_idx_sorted:\n",
    "    print(f\"  Piece {orig_idx}: length {len(ids)}\")\n",
    "print()\n",
    "\n",
    "print(\"üí° After processing, results are restored to original order using indices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268b86e",
   "metadata": {},
   "source": [
    "## üìù Summary: What Did We Learn?\n",
    "\n",
    "### 1. **The Script's Flow**:\n",
    "```\n",
    "Load JSON ‚Üí For each article ‚Üí For each batch:\n",
    "  1. Tokenize pieces\n",
    "  2. Identify target spans\n",
    "  3. Pad to same length\n",
    "  4. Create attention mask\n",
    "  5. Run model forward pass\n",
    "  6. Compute cross-entropy loss\n",
    "  7. Extract target surprisals\n",
    "  8. Sum per piece\n",
    "‚Üí Save to scores.json + eval.txt\n",
    "```\n",
    "\n",
    "### 2. **Key Concepts**:\n",
    "- **Input/Output alignment**: Input = text[:-1], Gold = text[1:]\n",
    "- **Target spans**: Where to extract surprisals from loss tensor\n",
    "- **Batching**: Process multiple pieces together for speed\n",
    "- **Padding**: Align sequences to same length (using -100)\n",
    "- **Attention mask**: Tell model to ignore padding positions\n",
    "- **Perplexity**: Average uncertainty = exp(mean_loss)\n",
    "\n",
    "### 3. **Why This Matters**:\n",
    "- Processes 212,000 pieces in reasonable time (~1-2 hours on GPU)\n",
    "- Outputs scores.json: article ‚Üí list of surprisals\n",
    "- These surprisals feed into statistical analysis (Part 5!)\n",
    "\n",
    "### 4. **The Numbers**:\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total pieces | ~212,000 |\n",
    "| Batch size | 50 |\n",
    "| Number of batches | ~4,240 |\n",
    "| Loss function | CrossEntropyLoss (nats) |\n",
    "| Output surprisal | Summed per word |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ready for Part 5?\n",
    "\n",
    "**Next**: Statistical analysis with `dundee.py` - how do we use these surprisals to compute PPP?\n",
    "\n",
    "### Check Your Understanding:\n",
    "1. Why do we use batching? **‚Üí Speed up computation (process multiple pieces at once)**\n",
    "2. What is padding? **‚Üí Adding -100 to align sequences to same length**\n",
    "3. Why input_ids[:-1]? **‚Üí Model at position i predicts token i+1**\n",
    "4. What is a target span? **‚Üí Positions where target tokens are located**\n",
    "5. What is perplexity? **‚Üí exp(mean_loss) - measures average uncertainty**\n",
    "6. Why sort by length? **‚Üí Minimize padding, faster computation**\n",
    "\n",
    "Tell me when you're ready for Part 5! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227beab3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
